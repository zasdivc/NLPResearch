{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f60081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "\n",
    "df = pd.read_csv('ChnSentiCorp_htl_all_translated.csv')\n",
    "df['review'] = df['review'].apply(str)\n",
    "df['translated_review'] = df['translated_review'].apply(str)\n",
    "\n",
    "import re\n",
    "def clean_sentence(sen):\n",
    "    clean_sen = re.sub(r'[^\\w\\s]', '', sen)\n",
    "    return clean_sen\n",
    "\n",
    "df['review'] = df['review'].apply(clean_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to keep only Chinese characters in a string\n",
    "def keep_chinese(text):\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\s]')\n",
    "    chinese_only = pattern.sub('', text)\n",
    "    return chinese_only.strip()\n",
    "\n",
    "# Segment the Chinese words using Jieba and keep only the Chinese characters in each word\n",
    "df['words-chinese'] = df['review'].apply(lambda x: [keep_chinese(word) for word in jieba.cut(x, cut_all=False) if keep_chinese(word) != ''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84de6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hangao/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 100 and the array at index 6374 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m tf_idf_reshaped \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf-idf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Combine the reshaped 'tf-idf' values and 'word2vec' values\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((tf_idf_reshaped, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword2vec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 100 and the array at index 6374 has size 1"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of TF-IDF values for each tokenized sentence\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df['words-chinese'].apply(lambda x: ' '.join(x)))\n",
    "df['tf-idf'] = np.mean(X_tfidf.toarray(), axis=1)\n",
    "\n",
    "# Train a Word2Vec model and calculate the mean of word embeddings for each tokenized sentence\n",
    "word2vec_model = Word2Vec(sentences=df['words-chinese'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "df['word2vec'] = df['words-chinese'].apply(lambda x: np.mean([word2vec_model.wv[word] for word in x], axis=0))\n",
    "\n",
    "# Reshape the 'tf-idf' values array to have the same number of columns as the 'word2vec' values array\n",
    "tf_idf_reshaped = df['tf-idf'].values.reshape(-1, 1)\n",
    "\n",
    "# Combine the reshaped 'tf-idf' values and 'word2vec' values\n",
    "X = np.hstack((tf_idf_reshaped, np.vstack(df['word2vec'].values)))\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate and report the F1 score, recall, and accuracy\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('F1 Score:', f1)\n",
    "print('Recall:', recall)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97b5c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hangao/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate all the words in df['words-chinese'] into a single string\n",
    "chinese_words = ' '.join(word for review in df['words-chinese'] for word in review)\n",
    "\n",
    "# Create the TfidfVectorizer object with the vocabulary set to the unique words in the concatenated string\n",
    "vectorizer = TfidfVectorizer(vocabulary=set(chinese_words.split()))\n",
    "\n",
    "# Fit the vectorizer on the concatenated string\n",
    "vectorizer.fit([chinese_words])\n",
    "\n",
    "# Compute the TF-IDF features for the df['review'] column\n",
    "tfidf_features_chinese = vectorizer.transform(df['review'])\n",
    "\n",
    "# Get the feature names from the vectorizer object\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Create a pandas DataFrame from the sparse matrix of the TF-IDF features\n",
    "df_new_chinese = pd.DataFrame.sparse.from_spmatrix(tfidf_features_chinese, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13c5ee1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1358587"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chinese_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c6910ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "7761    0\n",
       "7762    0\n",
       "7763    0\n",
       "7764    0\n",
       "7765    0\n",
       "Name: nonzero_columns, Length: 7766, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_chinese['nonzero_columns'] = df_new_chinese.apply(lambda x: (x != 0).sum(), axis=1)\n",
    "df_new_chinese['nonzero_columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab9e7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nonzero_rows = (df_new_chinese != 0).sum(axis=1)\n",
    "count_nonzero_rows = (nonzero_rows != 0).sum()\n",
    "print(count_nonzero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cce06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec and create the feature for Chinese sentences\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "sentences = df['words-chinese']\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "w2v_features = pd.DataFrame([\n",
    "    np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
    "    for sentence in sentences if np.any([model.wv[word] for word in sentence if word in model.wv])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da811e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6   \\\n",
      "0    -0.196257  0.277021  0.104843  0.082773 -0.114960 -0.236471  0.165146   \n",
      "1    -0.055645  0.227108  0.184772 -0.663236 -0.845417  0.232540 -0.069390   \n",
      "2    -0.032474  0.161153  0.191869  0.072029 -0.152428 -0.174706  0.220331   \n",
      "3    -0.078297  0.171496  0.304925 -0.169283 -0.253415 -0.310697  0.176672   \n",
      "4    -0.430415  0.490308  0.104770  0.000661 -0.397055 -0.230514  0.280223   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7759 -0.220421  0.337080  0.093458  0.112587 -0.195769 -0.394888  0.243077   \n",
      "7760 -0.175233  0.161407  0.203813 -0.047241 -0.182284 -0.178380  0.211565   \n",
      "7761 -0.133019  0.172838  0.315441 -0.106743 -0.273937 -0.104692  0.160264   \n",
      "7762 -0.203847  0.337881  0.150153  0.015690 -0.034044 -0.382851  0.254389   \n",
      "7763 -0.009343  0.031065  0.197352 -0.047649  0.266214 -0.216281  0.183524   \n",
      "\n",
      "            7         8         9   ...        90        91        92  \\\n",
      "0     0.326722 -0.048707 -0.182748  ...  0.417210  0.064948  0.186239   \n",
      "1    -0.033324 -0.627620 -0.566193  ...  0.468323 -0.348685 -0.403379   \n",
      "2     0.641093 -0.126274 -0.369099  ...  0.523676  0.093962  0.279360   \n",
      "3     0.545793 -0.399102 -0.343289  ...  0.434682 -0.020829  0.131890   \n",
      "4     0.742599 -0.274986 -0.353644  ...  0.691284 -0.115517  0.346502   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "7759  0.618875 -0.191686 -0.384155  ...  0.385299 -0.032709  0.040456   \n",
      "7760  0.680159 -0.230773 -0.356700  ...  0.480198  0.054974  0.281885   \n",
      "7761  0.581576 -0.164336 -0.442384  ...  0.477706  0.002244  0.255737   \n",
      "7762  0.844987 -0.162609 -0.351395  ...  0.336982 -0.085893  0.175200   \n",
      "7763  0.757787  0.033757 -0.187790  ...  0.432874  0.037603  0.456757   \n",
      "\n",
      "            93        94        95        96        97        98        99  \n",
      "0     0.036413  0.557214  0.322344  0.350740 -0.026661 -0.095189 -0.013301  \n",
      "1    -0.683436  0.298981  0.527175  0.788333 -0.474510  0.187530 -0.226159  \n",
      "2    -0.289585  0.572958  0.260153  0.387738 -0.240178  0.095738 -0.025784  \n",
      "3    -0.317654  0.623797  0.115826  0.506254 -0.253995  0.145028 -0.032491  \n",
      "4    -0.338203  0.820694  0.374706  0.594764 -0.446073 -0.153820 -0.015542  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "7759 -0.019797  0.544554  0.312237  0.274959 -0.210555  0.096943 -0.006190  \n",
      "7760 -0.219121  0.421724  0.391757  0.158413 -0.315078  0.024508 -0.067707  \n",
      "7761 -0.372034  0.492289  0.302254  0.322847 -0.312064  0.104685 -0.035804  \n",
      "7762 -0.054321  0.366744  0.289503  0.112336 -0.403006  0.028984 -0.076479  \n",
      "7763 -0.048794  0.233834  0.482082 -0.134805 -0.238947 -0.007576 -0.255208  \n",
      "\n",
      "[7764 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d59849dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'scipy.sparse.csr.csr_matrix'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m w2v_features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(w2v_features)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# concatenating the dataframes\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtfidf_features_chinese\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2v_features_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py:437\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[1;32m    433\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    434\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly Series and DataFrame objs are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         )\n\u001b[0;32m--> 437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    439\u001b[0m     ndims\u001b[38;5;241m.\u001b[39madd(obj\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# get the sample\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# want the highest ndim that we have, and must be non-empty\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# unless all objs are empty\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'scipy.sparse.csr.csr_matrix'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# converting csr_matrix to dataframe\n",
    "w2v_features_df = pd.DataFrame(w2v_features)\n",
    "\n",
    "# concatenating the dataframes\n",
    "combined_features = pd.concat([tfidf_features_chinese, w2v_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc47e277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hangao/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7764, 7766]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_tfidf_chinese, w2v_features], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Split the data into train and test sets\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Train a logistic regression model and evaluate the accuracy on the test set\u001b[39;00m\n\u001b[1;32m     60\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2422\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 378\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    335\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7764, 7766]"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from snownlp import SnowNLP\n",
    "# from collections import Counter\n",
    "# import jieba\n",
    "# import re\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from gensim.models import Word2Vec\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Read in the data\n",
    "# df = pd.read_csv('ChnSentiCorp_htl_all_translated.csv')\n",
    "# df['review'] = df['review'].apply(str)\n",
    "# df['translated_review'] = df['translated_review'].apply(str)\n",
    "\n",
    "# # Clean the sentences\n",
    "# def clean_sentence(sen):\n",
    "#     clean_sen = re.sub(r'[^\\w\\s]', '', sen)\n",
    "#     return clean_sen\n",
    "# df['review'] = df['review'].apply(clean_sentence)\n",
    "\n",
    "# # Define a function to keep only Chinese characters in a string\n",
    "# def keep_chinese(text):\n",
    "#     pattern = re.compile(r'[^\\u4e00-\\u9fff\\s]')\n",
    "#     chinese_only = pattern.sub('', text)\n",
    "#     return chinese_only.strip()\n",
    "\n",
    "# # Segment the Chinese words using Jieba and keep only the Chinese characters in each word\n",
    "# df['chinese-words'] = df['review'].apply(lambda x: [keep_chinese(word) for word in jieba.cut(x, cut_all=False) if keep_chinese(word) != ''])\n",
    "\n",
    "# # Create the TfidfVectorizer object with the vocabulary set to the unique words in the concatenated string\n",
    "# chinese_words = ' '.join(word for review in df['chinese-words'] for word in review)\n",
    "# vectorizer = TfidfVectorizer(vocabulary=set(chinese_words.split()))\n",
    "\n",
    "# # Fit the vectorizer on the concatenated string and compute the TF-IDF features for the df['chinese-words'] column\n",
    "# vectorizer.fit([chinese_words])\n",
    "# tfidf_features_chinese = vectorizer.transform([' '.join(words) for words in df['chinese-words']])\n",
    "\n",
    "# # Get the feature names from the vectorizer object and create a pandas DataFrame from the sparse matrix of the TF-IDF features\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "# df_tfidf_chinese = pd.DataFrame.sparse.from_spmatrix(tfidf_features_chinese, columns=feature_names)\n",
    "\n",
    "# # Train Word2Vec and create the feature for Chinese sentences\n",
    "# sentences = df['chinese-words']\n",
    "# model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "# w2v_features = pd.DataFrame([\n",
    "#     np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
    "#     for sentence in sentences if np.any([model.wv[word] for word in sentence if word in model.wv])\n",
    "# ])\n",
    "\n",
    "# # Concatenate the dataframes\n",
    "# combined_features = pd.concat([df_tfidf_chinese, w2v_features], axis=1)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(w2v_features, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a logistic regression model and evaluate the accuracy on the test set\n",
    "# lr_model = LogisticRegression()\n",
    "# lr_model.fit(X_train, y_train)\n",
    "# y_pred = lr_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6afd071a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                0\n",
       "review               0\n",
       "translated_review    0\n",
       "chinese-words        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a36cee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "df = pd.read_csv('ChnSentiCorp_htl_all_translated.csv')\n",
    "df['review'] = df['review'].apply(str)\n",
    "df['translated_review'] = df['translated_review'].apply(str)\n",
    "\n",
    "def clean_sentence(sen):\n",
    "    clean_sen = re.sub(r'[^\\w\\s]', '', sen)\n",
    "    return clean_sen\n",
    "\n",
    "df['review'] = df['review'].apply(clean_sentence)\n",
    "\n",
    "def keep_chinese(text):\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fff\\s]')\n",
    "    chinese_only = pattern.sub('', text)\n",
    "    return chinese_only.strip()\n",
    "\n",
    "df['chinese-words'] = df['review'].apply(\n",
    "    lambda x: [keep_chinese(word) for word in jieba.cut(x, cut_all=False) if keep_chinese(word) != '']\n",
    ")\n",
    "\n",
    "sentences = df['chinese-words']\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "valid_indices = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if np.any([model.wv[word] for word in sentence if word in model.wv]):\n",
    "        valid_indices.append(i)\n",
    "\n",
    "df_filtered = df.iloc[valid_indices]\n",
    "\n",
    "w2v_features = []\n",
    "for sentence in sentences:\n",
    "    if np.any([model.wv[word] for word in sentence if word in model.wv]):\n",
    "        w2v_features.append(np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0))\n",
    "\n",
    "w2v_features = pd.DataFrame(w2v_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(w2v_features, df_filtered['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a3b3cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8235672891178365\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model and evaluate the accuracy on the test set\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e72d89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8139085640695428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM model and evaluate the accuracy on the test set\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4229971",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train a Naive Bayes model and evaluate the accuracy on the test set\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nb_model \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m nb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      7\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:690\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    688\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:863\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1249\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1246\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train a Naive Bayes model and evaluate the accuracy on the test set\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred = nb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53aae4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8235672891178365\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model with L2 regularization and evaluate the accuracy on the test set\n",
    "lr_model = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e04f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
